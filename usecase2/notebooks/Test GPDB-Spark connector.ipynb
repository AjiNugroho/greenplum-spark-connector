{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%AddJar` not found.\n"
     ]
    }
   ],
   "source": [
    "%AddJar file:///home/jovyan/work/data/postgresql-42.2.2.jar -f\n",
    "\n",
    "    \n",
    "    #%AddJar file:///home/jovyan/work/data/greenplum-spark_2.11-1.4.0-alpha.jar -f\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-cfa2977469de>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-cfa2977469de>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    import org.apache.spark.sql.types.{StructField, StructType}\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types.DataType._\n",
    "import org.apache.spark.sql.types.{StructField, StructType}\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val data = Seq(\n",
    "  Row(1, \"a\"),\n",
    "  Row(5, \"z\")\n",
    ")\n",
    "\n",
    "val schema = StructType(\n",
    "  List(\n",
    "    StructField(\"num\", IntegerType, true),\n",
    "    StructField(\"letter\", StringType, true)\n",
    "  )\n",
    ")\n",
    "\n",
    "val df = spark.createDataFrame(\n",
    "  spark.sparkContext.parallelize(data),\n",
    "  schema\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0b52e24bf5ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Class' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-55404b05eb67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mClass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"org.postgresql.Driver\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Class' is not defined"
     ]
    }
   ],
   "source": [
    "Class.forName(\"org.postgresql.Driver\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-0ff62d4ac697>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-0ff62d4ac697>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    val dataFrame = spark.read.format(\"io.pivotal.greenplum.spark.GreenplumRelationProvider\")\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "val dataFrame = spark.read.format(\"io.pivotal.greenplum.spark.GreenplumRelationProvider\")\n",
    ".option(\"dbtable\", \"usertable\")\n",
    ".option(\"url\", \"jdbc:postgresql://gpdbsne/basic_db\")\n",
    ".option(\"user\", \"gpadmin\")\n",
    ".option(\"password\", \"pivotal\")\n",
    ".option(\"partitionColumn\", \"id\")\n",
    ".load()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-1bbc6f32e55a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-1bbc6f32e55a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    val opts = Map(\"url\" -> \"jdbc:postgresql://gpdbsne/basic_db?user=gpadmin&password=pivotal\",\"dbtable\" -> \"usertable\")\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "val opts = Map(\"url\" -> \"jdbc:postgresql://gpdbsne/basic_db?user=gpadmin&password=pivotal\",\"dbtable\" -> \"usertable\")\n",
    "val df = spark.read.format(\"jdbc\").options(opts).load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Name: java.lang.InstantiationException\n",
       "Message: org.apache.spark.sql.execution.datasources.jdbc.DriverWrapper\n",
       "StackTrace:   at org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:53)\n",
       "  at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1.apply(JdbcUtils.scala:55)\n",
       "  at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$createConnectionFactory$1.apply(JdbcUtils.scala:54)\n",
       "  at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:56)\n",
       "  at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation.<init>(JDBCRelation.scala:115)\n",
       "  at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:52)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:340)\n",
       "  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n",
       "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n",
       "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:164)\n",
       "  ... 50 elided\n",
       "Caused by: java.lang.NoSuchMethodException: org.apache.spark.sql.execution.datasources.jdbc.DriverWrapper.<init>()\n",
       "  at java.lang.Class.getConstructor0(Class.java:3082)\n",
       "  at java.lang.Class.newInstance(Class.java:412)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val jdbcDF = spark.read\n",
    "  .format(\"jdbc\")\n",
    "  .option(\"url\", \"jdbc:postgresql://gpdbsne:5432/basic_db\")\n",
    "  .option(\"dbtable\", \"usertable\")\n",
    "  .option(\"user\", \"gpadmin\")\n",
    "  .option(\"password\", \"pivotal\")\n",
    "  .option(\"partitionColumn\", \"id\")\n",
    "  .option(\"lowerBound\", \"0\")\n",
    "  .option(\"upperBound\", \"1000\")\n",
    "  .option(\"numPartitions\", \"100\")\n",
    "  .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
